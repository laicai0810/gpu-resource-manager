#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Enterprise GPU Resource Management Framework
ä¼ä¸šçº§GPUèµ„æºç®¡ç†æ¡†æ¶ v3.0

Features:
- å®Œæ•´çš„APIæ¥å£è®¾è®¡
- æ™ºèƒ½ä»»åŠ¡è°ƒåº¦ä¸è´Ÿè½½å‡è¡¡
- å®æ—¶ç›‘æ§ä¸å¯è§†åŒ–
- è‡ªåŠ¨å†…å­˜ç®¡ç†ä¸ä¼˜åŒ–
- åˆ†å¸ƒå¼ä»»åŠ¡å¤„ç†
- è¿›ç¨‹çº§èµ„æºè¿½è¸ª
"""

import subprocess
import os
import sys
import json
import time
import psutil
import threading
import queue
import logging
import traceback
import numpy as np
import pandas as pd
from typing import List, Dict, Optional, Callable, Any, Union, Tuple
from dataclasses import dataclass, field, asdict
from enum import Enum
from datetime import datetime, timedelta
from collections import defaultdict, deque
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor, Future
from functools import wraps, lru_cache
import pickle
import gc
import weakref
import signal
import atexit

try:
    import cupy as cp
    import pynvml
    CUDA_AVAILABLE = True
except ImportError:
    CUDA_AVAILABLE = False
    print("âš ï¸ CUDA/CuPy not available. Running in CPU-only mode.")

try:
    import gradio as gr
    GRADIO_AVAILABLE = True
except ImportError:
    GRADIO_AVAILABLE = False
    print("âš ï¸ Gradio not available. Web UI disabled.")

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(name)s: %(message)s',
    handlers=[
        logging.FileHandler('gpu_manager.log'),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)


class TaskPriority(Enum):
    """ä»»åŠ¡ä¼˜å…ˆçº§æšä¸¾"""
    CRITICAL = 5
    HIGH = 4
    NORMAL = 3
    LOW = 2
    IDLE = 1


class TaskStatus(Enum):
    """ä»»åŠ¡çŠ¶æ€æšä¸¾"""
    PENDING = "pending"
    RUNNING = "running"
    COMPLETED = "completed"
    FAILED = "failed"
    CANCELLED = "cancelled"


class SchedulingStrategy(Enum):
    """è°ƒåº¦ç­–ç•¥æšä¸¾"""
    ROUND_ROBIN = "round_robin"
    LEAST_LOADED = "least_loaded"
    MEMORY_FIRST = "memory_first"
    PERFORMANCE_FIRST = "performance_first"
    ADAPTIVE = "adaptive"


@dataclass
class GPUInfo:
    """GPUä¿¡æ¯æ•°æ®ç±»"""
    index: int
    name: str
    uuid: str
    memory_total: int  # MB
    memory_free: int   # MB
    memory_used: int   # MB
    utilization: float  # %
    temperature: float  # Â°C
    power_usage: float  # W
    processes: List[Dict] = field(default_factory=list)
    compute_capability: Tuple[int, int] = (0, 0)
    
    @property
    def memory_usage_percent(self) -> float:
        return (self.memory_used / self.memory_total * 100) if self.memory_total > 0 else 0
    
    @property
    def available_memory_gb(self) -> float:
        return self.memory_free / 1024.0


@dataclass
class GPUTask:
    """GPUä»»åŠ¡æ•°æ®ç±»"""
    task_id: str
    func: Callable
    args: tuple = ()
    kwargs: dict = field(default_factory=dict)
    priority: TaskPriority = TaskPriority.NORMAL
    required_memory_gb: float = 1.0
    max_retries: int = 3
    timeout: Optional[float] = None
    callback: Optional[Callable] = None
    error_callback: Optional[Callable] = None
    
    # è¿è¡Œæ—¶å±æ€§
    status: TaskStatus = TaskStatus.PENDING
    assigned_gpu: Optional[int] = None
    start_time: Optional[datetime] = None
    end_time: Optional[datetime] = None
    result: Any = None
    error: Optional[Exception] = None
    retry_count: int = 0
    
    @property
    def execution_time(self) -> Optional[timedelta]:
        if self.start_time and self.end_time:
            return self.end_time - self.start_time
        return None


class GPUProcessMonitor:
    """GPUè¿›ç¨‹ç›‘æ§å™¨"""
    
    def __init__(self):
        self.process_cache = {}
        self._lock = threading.Lock()
        
    def get_gpu_processes(self, gpu_index: int) -> List[Dict]:
        """è·å–æŒ‡å®šGPUä¸Šçš„è¿›ç¨‹ä¿¡æ¯"""
        processes = []
        if not CUDA_AVAILABLE:
            return processes
            
        try:
            handle = pynvml.nvmlDeviceGetHandleByIndex(gpu_index)
            procs = pynvml.nvmlDeviceGetComputeRunningProcesses(handle)
            
            for proc in procs:
                pid = proc.pid
                used_memory = proc.usedGpuMemory // (1024 * 1024)  # MB
                
                # è·å–è¿›ç¨‹è¯¦ç»†ä¿¡æ¯
                try:
                    p = psutil.Process(pid)
                    with self._lock:
                        self.process_cache[pid] = {
                            'pid': pid,
                            'name': p.name(),
                            'username': p.username(),
                            'create_time': p.create_time(),
                            'memory_mb': used_memory,
                            'cpu_percent': p.cpu_percent(),
                            'memory_percent': p.memory_percent(),
                            'status': p.status(),
                            'cmdline': ' '.join(p.cmdline()[:50])  # é™åˆ¶å‘½ä»¤è¡Œé•¿åº¦
                        }
                except (psutil.NoSuchProcess, psutil.AccessDenied):
                    pass
                    
                if pid in self.process_cache:
                    processes.append(self.process_cache[pid])
                    
        except Exception as e:
            logger.error(f"Failed to get GPU processes: {e}")
            
        return processes
    
    def get_current_process_gpu_usage(self) -> Dict[int, int]:
        """è·å–å½“å‰è¿›ç¨‹çš„GPUä½¿ç”¨æƒ…å†µ"""
        current_pid = os.getpid()
        gpu_usage = {}
        
        if not CUDA_AVAILABLE:
            return gpu_usage
            
        try:
            for i in range(pynvml.nvmlDeviceGetCount()):
                handle = pynvml.nvmlDeviceGetHandleByIndex(i)
                procs = pynvml.nvmlDeviceGetComputeRunningProcesses(handle)
                
                for proc in procs:
                    if proc.pid == current_pid:
                        gpu_usage[i] = proc.usedGpuMemory // (1024 * 1024)
                        
        except Exception as e:
            logger.error(f"Failed to get current process GPU usage: {e}")
            
        return gpu_usage


class IntelligentScheduler:
    """æ™ºèƒ½ä»»åŠ¡è°ƒåº¦å™¨"""
    
    def __init__(self, strategy: SchedulingStrategy = SchedulingStrategy.ADAPTIVE):
        self.strategy = strategy
        self.task_history = deque(maxlen=1000)
        self.gpu_load_history = defaultdict(lambda: deque(maxlen=100))
        self._lock = threading.Lock()
        
    def select_gpu(self, task: GPUTask, available_gpus: List[GPUInfo]) -> Optional[int]:
        """æ ¹æ®ç­–ç•¥é€‰æ‹©æœ€ä½³GPU"""
        if not available_gpus:
            return None
            
        with self._lock:
            if self.strategy == SchedulingStrategy.ROUND_ROBIN:
                return self._round_robin_select(available_gpus)
            elif self.strategy == SchedulingStrategy.LEAST_LOADED:
                return self._least_loaded_select(available_gpus)
            elif self.strategy == SchedulingStrategy.MEMORY_FIRST:
                return self._memory_first_select(task, available_gpus)
            elif self.strategy == SchedulingStrategy.PERFORMANCE_FIRST:
                return self._performance_first_select(available_gpus)
            elif self.strategy == SchedulingStrategy.ADAPTIVE:
                return self._adaptive_select(task, available_gpus)
                
        return available_gpus[0].index
    
    def _round_robin_select(self, gpus: List[GPUInfo]) -> int:
        """è½®è¯¢é€‰æ‹©"""
        # ç®€å•å®ç°ï¼šé€‰æ‹©è´Ÿè½½æœ€ä½çš„GPU
        return min(gpus, key=lambda g: len(g.processes)).index
    
    def _least_loaded_select(self, gpus: List[GPUInfo]) -> int:
        """é€‰æ‹©è´Ÿè½½æœ€ä½çš„GPU"""
        return min(gpus, key=lambda g: g.utilization).index
    
    def _memory_first_select(self, task: GPUTask, gpus: List[GPUInfo]) -> int:
        """ä¼˜å…ˆé€‰æ‹©å†…å­˜å……è¶³çš„GPU"""
        suitable_gpus = [g for g in gpus if g.available_memory_gb >= task.required_memory_gb]
        if suitable_gpus:
            return max(suitable_gpus, key=lambda g: g.memory_free).index
        return gpus[0].index
    
    def _performance_first_select(self, gpus: List[GPUInfo]) -> int:
        """ä¼˜å…ˆé€‰æ‹©æ€§èƒ½æœ€å¥½çš„GPU"""
        # ç»¼åˆè€ƒè™‘åˆ©ç”¨ç‡ã€æ¸©åº¦å’ŒåŠŸè€—
        def gpu_score(gpu: GPUInfo) -> float:
            util_score = 100 - gpu.utilization
            temp_score = max(0, 85 - gpu.temperature)  # 85Â°Cä¸ºè­¦æˆ’æ¸©åº¦
            power_score = 100 - (gpu.power_usage / 300 * 100)  # å‡è®¾300Wä¸ºæœ€å¤§åŠŸè€—
            return util_score * 0.5 + temp_score * 0.3 + power_score * 0.2
            
        return max(gpus, key=gpu_score).index
    
    def _adaptive_select(self, task: GPUTask, gpus: List[GPUInfo]) -> int:
        """è‡ªé€‚åº”é€‰æ‹©ç­–ç•¥"""
        # æ ¹æ®ä»»åŠ¡ç‰¹å¾å’Œå†å²æ•°æ®æ™ºèƒ½é€‰æ‹©
        if task.required_memory_gb > 8:  # å¤§å†…å­˜ä»»åŠ¡
            return self._memory_first_select(task, gpus)
        elif task.priority == TaskPriority.CRITICAL:  # å…³é”®ä»»åŠ¡
            return self._performance_first_select(gpus)
        else:  # æ™®é€šä»»åŠ¡
            return self._least_loaded_select(gpus)
    
    def update_history(self, task: GPUTask, gpu_info: GPUInfo):
        """æ›´æ–°è°ƒåº¦å†å²"""
        with self._lock:
            self.task_history.append({
                'task_id': task.task_id,
                'gpu_index': gpu_info.index,
                'timestamp': datetime.now(),
                'execution_time': task.execution_time,
                'memory_used': task.required_memory_gb,
                'status': task.status
            })
            
            self.gpu_load_history[gpu_info.index].append({
                'timestamp': datetime.now(),
                'utilization': gpu_info.utilization,
                'memory_used': gpu_info.memory_used,
                'temperature': gpu_info.temperature
            })


class GPUResourcePool:
    """GPUèµ„æºæ± ç®¡ç†å™¨"""
    
    def __init__(self, min_free_memory_gb: float = 2.0):
        self.min_free_memory_gb = min_free_memory_gb
        self.gpu_locks = {}
        self.allocation_map = {}
        self._lock = threading.Lock()
        self.process_monitor = GPUProcessMonitor()
        
        if CUDA_AVAILABLE:
            pynvml.nvmlInit()
            self._init_gpu_locks()
            
    def _init_gpu_locks(self):
        """åˆå§‹åŒ–GPUé”"""
        gpu_count = pynvml.nvmlDeviceGetCount() if CUDA_AVAILABLE else 0
        for i in range(gpu_count):
            self.gpu_locks[i] = threading.Semaphore(1)
            
    def get_all_gpu_info(self) -> List[GPUInfo]:
        """è·å–æ‰€æœ‰GPUçš„è¯¦ç»†ä¿¡æ¯"""
        gpus = []
        if not CUDA_AVAILABLE:
            return gpus
            
        try:
            device_count = pynvml.nvmlDeviceGetCount()
            
            for i in range(device_count):
                handle = pynvml.nvmlDeviceGetHandleByIndex(i)
                
                # åŸºç¡€ä¿¡æ¯
                name = pynvml.nvmlDeviceGetName(handle).decode('utf-8')
                uuid = pynvml.nvmlDeviceGetUUID(handle).decode('utf-8')
                
                # å†…å­˜ä¿¡æ¯
                mem_info = pynvml.nvmlDeviceGetMemoryInfo(handle)
                memory_total = mem_info.total // (1024 * 1024)
                memory_free = mem_info.free // (1024 * 1024)
                memory_used = mem_info.used // (1024 * 1024)
                
                # åˆ©ç”¨ç‡
                utilization = pynvml.nvmlDeviceGetUtilizationRates(handle).gpu
                
                # æ¸©åº¦
                try:
                    temperature = pynvml.nvmlDeviceGetTemperature(handle, pynvml.NVML_TEMPERATURE_GPU)
                except:
                    temperature = 0.0
                
                # åŠŸè€—
                try:
                    power_usage = pynvml.nvmlDeviceGetPowerUsage(handle) / 1000.0  # mW to W
                except:
                    power_usage = 0.0
                
                # è®¡ç®—èƒ½åŠ›
                try:
                    major, minor = pynvml.nvmlDeviceGetCudaComputeCapability(handle)
                    compute_capability = (major, minor)
                except:
                    compute_capability = (0, 0)
                
                # è¿›ç¨‹ä¿¡æ¯
                processes = self.process_monitor.get_gpu_processes(i)
                
                gpu_info = GPUInfo(
                    index=i,
                    name=name,
                    uuid=uuid,
                    memory_total=memory_total,
                    memory_free=memory_free,
                    memory_used=memory_used,
                    utilization=utilization,
                    temperature=temperature,
                    power_usage=power_usage,
                    processes=processes,
                    compute_capability=compute_capability
                )
                
                gpus.append(gpu_info)
                
        except Exception as e:
            logger.error(f"Failed to get GPU info: {e}")
            
        return gpus
    
    def allocate_gpu(self, required_memory_gb: float, 
                      timeout: Optional[float] = None) -> Optional[int]:
        """åˆ†é…GPUèµ„æº"""
        start_time = time.time()
        
        while True:
            with self._lock:
                gpus = self.get_all_gpu_info()
                available_gpus = [
                    gpu for gpu in gpus 
                    if gpu.available_memory_gb >= max(required_memory_gb, self.min_free_memory_gb)
                    and gpu.index not in self.allocation_map
                ]
                
                if available_gpus:
                    # é€‰æ‹©æœ€ä½³GPU
                    best_gpu = max(available_gpus, key=lambda g: g.memory_free)
                    gpu_index = best_gpu.index
                    
                    if self.gpu_locks[gpu_index].acquire(blocking=False):
                        self.allocation_map[gpu_index] = {
                            'allocated_at': datetime.now(),
                            'required_memory_gb': required_memory_gb,
                            'pid': os.getpid()
                        }
                        return gpu_index
                        
            # æ£€æŸ¥è¶…æ—¶
            if timeout and (time.time() - start_time) > timeout:
                return None
                
            time.sleep(0.1)
    
    def release_gpu(self, gpu_index: int):
        """é‡Šæ”¾GPUèµ„æº"""
        with self._lock:
            if gpu_index in self.allocation_map:
                del self.allocation_map[gpu_index]
                self.gpu_locks[gpu_index].release()
                
                # æ¸…ç†GPUå†…å­˜
                self.cleanup_gpu_memory(gpu_index)
    
    def cleanup_gpu_memory(self, gpu_index: int):
        """æ¸…ç†GPUå†…å­˜"""
        if not CUDA_AVAILABLE:
            return
            
        try:
            with cp.cuda.Device(gpu_index):
                gc.collect()
                cp.get_default_memory_pool().free_all_blocks()
                cp.get_default_pinned_memory_pool().free_all_blocks()
        except Exception as e:
            logger.error(f"Failed to cleanup GPU {gpu_index} memory: {e}")


class TaskExecutor:
    """ä»»åŠ¡æ‰§è¡Œå™¨"""
    
    def __init__(self, resource_pool: GPUResourcePool):
        self.resource_pool = resource_pool
        self.current_tasks = {}
        self._lock = threading.Lock()
        
    def execute_task(self, task: GPUTask, gpu_index: int) -> Any:
        """åœ¨æŒ‡å®šGPUä¸Šæ‰§è¡Œä»»åŠ¡"""
        task.assigned_gpu = gpu_index
        task.start_time = datetime.now()
        task.status = TaskStatus.RUNNING
        
        try:
            with self._lock:
                self.current_tasks[task.task_id] = task
                
            # è®¾ç½®GPUè®¾å¤‡
            if CUDA_AVAILABLE:
                cp.cuda.Device(gpu_index).use()
                
            # æ‰§è¡Œä»»åŠ¡
            if task.timeout:
                # ä½¿ç”¨è¶…æ—¶æ§åˆ¶
                import concurrent.futures
                with concurrent.futures.ThreadPoolExecutor(max_workers=1) as executor:
                    future = executor.submit(task.func, *task.args, **task.kwargs)
                    result = future.result(timeout=task.timeout)
            else:
                result = task.func(*task.args, **task.kwargs)
                
            task.result = result
            task.status = TaskStatus.COMPLETED
            task.end_time = datetime.now()
            
            # æ‰§è¡Œå›è°ƒ
            if task.callback:
                task.callback(result)
                
            return result
            
        except Exception as e:
            task.error = e
            task.status = TaskStatus.FAILED
            task.end_time = datetime.now()
            
            # æ‰§è¡Œé”™è¯¯å›è°ƒ
            if task.error_callback:
                task.error_callback(e)
                
            raise
            
        finally:
            with self._lock:
                if task.task_id in self.current_tasks:
                    del self.current_tasks[task.task_id]
                    
            # é‡Šæ”¾GPUèµ„æº
            self.resource_pool.release_gpu(gpu_index)


class GPUManager:
    """GPUç®¡ç†å™¨ä¸»ç±» - æä¾›å®Œæ•´çš„APIæ¥å£"""
    
    def __init__(self, 
                 min_free_memory_gb: float = 2.0,
                 scheduling_strategy: SchedulingStrategy = SchedulingStrategy.ADAPTIVE,
                 max_workers: int = 4,
                 enable_monitoring: bool = True):
        """
        åˆå§‹åŒ–GPUç®¡ç†å™¨
        
        Args:
            min_free_memory_gb: æœ€å°å¯ç”¨å†…å­˜è¦æ±‚
            scheduling_strategy: è°ƒåº¦ç­–ç•¥
            max_workers: æœ€å¤§å·¥ä½œçº¿ç¨‹æ•°
            enable_monitoring: æ˜¯å¦å¯ç”¨ç›‘æ§
        """
        self.resource_pool = GPUResourcePool(min_free_memory_gb)
        self.scheduler = IntelligentScheduler(scheduling_strategy)
        self.executor = TaskExecutor(self.resource_pool)
        self.task_queue = queue.PriorityQueue()
        self.max_workers = max_workers
        self.enable_monitoring = enable_monitoring
        
        self.running = False
        self.workers = []
        self.monitor_thread = None
        self.task_results = {}
        self.metrics = {
            'total_tasks': 0,
            'completed_tasks': 0,
            'failed_tasks': 0,
            'total_execution_time': timedelta(0),
            'gpu_utilization_history': defaultdict(list)
        }
        
        self._lock = threading.Lock()
        self._shutdown_event = threading.Event()
        
        # æ³¨å†Œæ¸…ç†å‡½æ•°
        atexit.register(self.shutdown)
        signal.signal(signal.SIGINT, self._signal_handler)
        signal.signal(signal.SIGTERM, self._signal_handler)
        
        logger.info("GPU Manager initialized successfully")
        
    def start(self):
        """å¯åŠ¨GPUç®¡ç†å™¨"""
        if self.running:
            return
            
        self.running = True
        
        # å¯åŠ¨å·¥ä½œçº¿ç¨‹
        for i in range(self.max_workers):
            worker = threading.Thread(target=self._worker_loop, 
                                    name=f"GPUWorker-{i}",
                                    daemon=True)
            worker.start()
            self.workers.append(worker)
            
        # å¯åŠ¨ç›‘æ§çº¿ç¨‹
        if self.enable_monitoring:
            self.monitor_thread = threading.Thread(target=self._monitor_loop,
                                                 name="GPUMonitor",
                                                 daemon=True)
            self.monitor_thread.start()
            
        logger.info(f"GPU Manager started with {self.max_workers} workers")
        
    def _worker_loop(self):
        """å·¥ä½œçº¿ç¨‹ä¸»å¾ªç¯"""
        while self.running:
            try:
                # è·å–ä»»åŠ¡ï¼ˆä¼˜å…ˆçº§ï¼Œä»»åŠ¡ï¼‰
                _, task = self.task_queue.get(timeout=1)
                
                # é‡è¯•é€»è¾‘
                while task.retry_count <= task.max_retries:
                    try:
                        # è·å–å¯ç”¨GPU
                        gpus = self.resource_pool.get_all_gpu_info()
                        available_gpus = [
                            gpu for gpu in gpus 
                            if gpu.available_memory_gb >= task.required_memory_gb
                        ]
                        
                        if not available_gpus:
                            logger.warning(f"No available GPU for task {task.task_id}")
                            time.sleep(1)
                            continue
                            
                        # é€‰æ‹©GPU
                        gpu_index = self.scheduler.select_gpu(task, available_gpus)
                        if gpu_index is None:
                            continue
                            
                        # åˆ†é…GPU
                        allocated_gpu = self.resource_pool.allocate_gpu(
                            task.required_memory_gb, 
                            timeout=30
                        )
                        
                        if allocated_gpu is not None:
                            # æ‰§è¡Œä»»åŠ¡
                            result = self.executor.execute_task(task, allocated_gpu)
                            
                            # æ›´æ–°ç»Ÿè®¡
                            with self._lock:
                                self.metrics['completed_tasks'] += 1
                                self.metrics['total_execution_time'] += task.execution_time
                                self.task_results[task.task_id] = result
                                
                            # æ›´æ–°è°ƒåº¦å†å²
                            gpu_info = next(g for g in gpus if g.index == allocated_gpu)
                            self.scheduler.update_history(task, gpu_info)
                            
                            break
                            
                    except Exception as e:
                        logger.error(f"Task {task.task_id} failed: {e}")
                        task.retry_count += 1
                        
                        if task.retry_count > task.max_retries:
                            task.status = TaskStatus.FAILED
                            task.error = e
                            with self._lock:
                                self.metrics['failed_tasks'] += 1
                                
                        time.sleep(1)
                        
            except queue.Empty:
                continue
            except Exception as e:
                logger.error(f"Worker loop error: {e}")
                
    def _monitor_loop(self):
        """ç›‘æ§çº¿ç¨‹ä¸»å¾ªç¯"""
        while self.running:
            try:
                # æ”¶é›†GPUçŠ¶æ€
                gpus = self.resource_pool.get_all_gpu_info()
                timestamp = datetime.now()
                
                for gpu in gpus:
                    self.metrics['gpu_utilization_history'][gpu.index].append({
                        'timestamp': timestamp,
                        'utilization': gpu.utilization,
                        'memory_usage': gpu.memory_usage_percent,
                        'temperature': gpu.temperature,
                        'power_usage': gpu.power_usage
                    })
                    
                    # ä¿ç•™æœ€è¿‘1å°æ—¶çš„æ•°æ®
                    history = self.metrics['gpu_utilization_history'][gpu.index]
                    cutoff_time = timestamp - timedelta(hours=1)
                    self.metrics['gpu_utilization_history'][gpu.index] = [
                        h for h in history if h['timestamp'] > cutoff_time
                    ]
                    
                time.sleep(5)  # 5ç§’é‡‡æ ·ä¸€æ¬¡
                
            except Exception as e:
                logger.error(f"Monitor loop error: {e}")
                
    def submit_task(self, 
                   func: Callable,
                   args: tuple = (),
                   kwargs: dict = None,
                   priority: TaskPriority = TaskPriority.NORMAL,
                   required_memory_gb: float = 1.0,
                   timeout: Optional[float] = None,
                   callback: Optional[Callable] = None,
                   error_callback: Optional[Callable] = None) -> str:
        """
        æäº¤GPUä»»åŠ¡
        
        Args:
            func: è¦æ‰§è¡Œçš„å‡½æ•°
            args: å‡½æ•°å‚æ•°
            kwargs: å‡½æ•°å…³é”®å­—å‚æ•°
            priority: ä»»åŠ¡ä¼˜å…ˆçº§
            required_memory_gb: æ‰€éœ€GPUå†…å­˜(GB)
            timeout: è¶…æ—¶æ—¶é—´(ç§’)
            callback: æˆåŠŸå›è°ƒå‡½æ•°
            error_callback: é”™è¯¯å›è°ƒå‡½æ•°
            
        Returns:
            task_id: ä»»åŠ¡ID
        """
        if kwargs is None:
            kwargs = {}
            
        # åˆ›å»ºä»»åŠ¡
        task_id = f"task_{int(time.time()*1000)}_{np.random.randint(1000)}"
        task = GPUTask(
            task_id=task_id,
            func=func,
            args=args,
            kwargs=kwargs,
            priority=priority,
            required_memory_gb=required_memory_gb,
            timeout=timeout,
            callback=callback,
            error_callback=error_callback
        )
        
        # åŠ å…¥é˜Ÿåˆ—
        priority_value = -task.priority.value  # è´Ÿå€¼ä½¿é«˜ä¼˜å…ˆçº§å…ˆæ‰§è¡Œ
        self.task_queue.put((priority_value, task))
        
        with self._lock:
            self.metrics['total_tasks'] += 1
            
        logger.info(f"Task {task_id} submitted with priority {priority.name}")
        return task_id
        
    def get_task_result(self, task_id: str, timeout: Optional[float] = None) -> Any:
        """
        è·å–ä»»åŠ¡ç»“æœ
        
        Args:
            task_id: ä»»åŠ¡ID
            timeout: è¶…æ—¶æ—¶é—´
            
        Returns:
            ä»»åŠ¡æ‰§è¡Œç»“æœ
        """
        start_time = time.time()
        
        while True:
            with self._lock:
                if task_id in self.task_results:
                    return self.task_results.pop(task_id)
                    
            if timeout and (time.time() - start_time) > timeout:
                raise TimeoutError(f"Task {task_id} result timeout")
                
            time.sleep(0.1)
            
    def batch_process(self,
                     data: Union[List, np.ndarray],
                     process_func: Callable,
                     batch_size: Optional[int] = None,
                     priority: TaskPriority = TaskPriority.NORMAL,
                     required_memory_gb: float = 1.0) -> List[Any]:
        """
        æ‰¹é‡å¤„ç†æ•°æ®
        
        Args:
            data: è¦å¤„ç†çš„æ•°æ®
            process_func: å¤„ç†å‡½æ•°
            batch_size: æ‰¹æ¬¡å¤§å°(Noneè¡¨ç¤ºè‡ªåŠ¨è®¡ç®—)
            priority: ä»»åŠ¡ä¼˜å…ˆçº§
            required_memory_gb: æ¯æ‰¹æ¬¡æ‰€éœ€å†…å­˜
            
        Returns:
            å¤„ç†ç»“æœåˆ—è¡¨
        """
        # è‡ªåŠ¨è®¡ç®—æ‰¹æ¬¡å¤§å°
        if batch_size is None:
            gpus = self.resource_pool.get_all_gpu_info()
            if gpus:
                avg_memory = np.mean([g.available_memory_gb for g in gpus])
                batch_size = max(1, int(len(data) * required_memory_gb / avg_memory))
            else:
                batch_size = 1
                
        # åˆ†æ‰¹å¤„ç†
        batches = [data[i:i+batch_size] for i in range(0, len(data), batch_size)]
        task_ids = []
        
        for i, batch in enumerate(batches):
            task_id = self.submit_task(
                func=process_func,
                args=(batch,),
                priority=priority,
                required_memory_gb=required_memory_gb
            )
            task_ids.append(task_id)
            
        # æ”¶é›†ç»“æœ
        results = []
        for task_id in task_ids:
            result = self.get_task_result(task_id)
            results.extend(result if isinstance(result, list) else [result])
            
        return results
        
    def get_status(self) -> Dict:
        """è·å–GPUç®¡ç†å™¨çŠ¶æ€"""
        with self._lock:
            gpus = self.resource_pool.get_all_gpu_info()
            
            return {
                'running': self.running,
                'workers': len(self.workers),
                'pending_tasks': self.task_queue.qsize(),
                'active_tasks': len(self.executor.current_tasks),
                'total_tasks': self.metrics['total_tasks'],
                'completed_tasks': self.metrics['completed_tasks'],
                'failed_tasks': self.metrics['failed_tasks'],
                'gpus': [asdict(gpu) for gpu in gpus],
                'current_process_usage': self.resource_pool.process_monitor.get_current_process_gpu_usage()
            }
    
    def print_status(self):
        """æ‰“å°GPUçŠ¶æ€ä¿¡æ¯"""
        status = self.get_status()
        
        print("\n" + "="*100)
        print(f"GPUèµ„æºç®¡ç†å™¨çŠ¶æ€ - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print("="*100)
        
        # åŸºæœ¬ä¿¡æ¯
        print(f"\nè¿è¡ŒçŠ¶æ€: {'âœ… è¿è¡Œä¸­' if status['running'] else 'âŒ å·²åœæ­¢'}")
        print(f"å·¥ä½œçº¿ç¨‹: {status['workers']}")
        print(f"å¾…å¤„ç†ä»»åŠ¡: {status['pending_tasks']}")
        print(f"æ‰§è¡Œä¸­ä»»åŠ¡: {status['active_tasks']}")
        print(f"æ€»ä»»åŠ¡æ•°: {status['total_tasks']} (å®Œæˆ: {status['completed_tasks']}, å¤±è´¥: {status['failed_tasks']})")
        
        # GPUä¿¡æ¯
        print("\n" + "-"*100)
        print(f"{'GPU':<4} {'åç§°':<30} {'æ˜¾å­˜ä½¿ç”¨':<20} {'åˆ©ç”¨ç‡':<10} {'æ¸©åº¦':<8} {'åŠŸè€—':<8} {'è¿›ç¨‹æ•°':<8}")
        print("-"*100)
        
        for gpu in status['gpus']:
            mem_usage = f"{gpu['memory_used']/1024:.1f}/{gpu['memory_total']/1024:.1f}GB"
            print(f"{gpu['index']:<4} {gpu['name']:<30} {mem_usage:<20} "
                  f"{gpu['utilization']:.0f}%{'':<7} {gpu['temperature']:.0f}Â°C{'':<5} "
                  f"{gpu['power_usage']:.0f}W{'':<5} {len(gpu['processes']):<8}")
        
        # å½“å‰è¿›ç¨‹GPUä½¿ç”¨
        if status['current_process_usage']:
            print(f"\nå½“å‰è¿›ç¨‹GPUä½¿ç”¨: {status['current_process_usage']}")
        
        print("="*100)
    
    def _signal_handler(self, signum, frame):
        """ä¿¡å·å¤„ç†å™¨"""
        logger.info(f"Received signal {signum}, shutting down...")
        self.shutdown()
        
    def shutdown(self):
        """å…³é—­GPUç®¡ç†å™¨"""
        if not self.running:
            return
            
        logger.info("Shutting down GPU Manager...")
        self.running = False
        self._shutdown_event.set()
        
        # ç­‰å¾…å·¥ä½œçº¿ç¨‹ç»“æŸ
        for worker in self.workers:
            worker.join(timeout=5)
            
        # ç­‰å¾…ç›‘æ§çº¿ç¨‹ç»“æŸ
        if self.monitor_thread:
            self.monitor_thread.join(timeout=5)
            
        logger.info("GPU Manager shutdown complete")


class GPUWebUI:
    """GPUç®¡ç†å™¨Webç•Œé¢"""
    
    def __init__(self, gpu_manager: GPUManager):
        self.gpu_manager = gpu_manager
        self.update_interval = 5
        
    def create_interface(self):
        """åˆ›å»ºGradioç•Œé¢"""
        if not GRADIO_AVAILABLE:
            logger.error("Gradio not available. Cannot create web UI.")
            return None
            
        with gr.Blocks(title="GPUèµ„æºç®¡ç†ç³»ç»Ÿ", theme=gr.themes.Soft()) as interface:
            gr.Markdown("# ğŸš€ GPUèµ„æºç®¡ç†ç³»ç»Ÿ")
            gr.Markdown("å®æ—¶ç›‘æ§å’Œç®¡ç†GPUèµ„æºï¼Œæ™ºèƒ½è°ƒåº¦è®¡ç®—ä»»åŠ¡")
            
            with gr.Tabs():
                # çŠ¶æ€ç›‘æ§æ ‡ç­¾
                with gr.TabItem("ğŸ“Š çŠ¶æ€ç›‘æ§"):
                    with gr.Row():
                        status_text = gr.JSON(label="ç³»ç»ŸçŠ¶æ€")
                        refresh_btn = gr.Button("ğŸ”„ åˆ·æ–°", variant="primary")
                    
                    with gr.Row():
                        gpu_usage_plot = gr.Plot(label="GPUä½¿ç”¨ç‡è¶‹åŠ¿")
                        memory_usage_plot = gr.Plot(label="æ˜¾å­˜ä½¿ç”¨è¶‹åŠ¿")
                    
                    with gr.Row():
                        temperature_plot = gr.Plot(label="æ¸©åº¦ç›‘æ§")
                        power_plot = gr.Plot(label="åŠŸè€—ç›‘æ§")
                
                # ä»»åŠ¡ç®¡ç†æ ‡ç­¾
                with gr.TabItem("ğŸ“‹ ä»»åŠ¡ç®¡ç†"):
                    with gr.Row():
                        with gr.Column(scale=3):
                            task_func = gr.Textbox(
                                label="ä»»åŠ¡å‡½æ•°", 
                                placeholder="ä¾‹å¦‚: numpy.dot",
                                value="numpy.random.randn"
                            )
                            task_args = gr.Textbox(
                                label="å‚æ•°", 
                                placeholder="(1000, 1000)",
                                value="(1000, 1000)"
                            )
                            required_memory = gr.Slider(
                                label="æ‰€éœ€æ˜¾å­˜(GB)", 
                                minimum=0.5, 
                                maximum=24, 
                                value=1.0,
                                step=0.5
                            )
                            priority = gr.Dropdown(
                                label="ä¼˜å…ˆçº§",
                                choices=["CRITICAL", "HIGH", "NORMAL", "LOW", "IDLE"],
                                value="NORMAL"
                            )
                        
                        with gr.Column(scale=1):
                            submit_btn = gr.Button("æäº¤ä»»åŠ¡", variant="primary")
                            task_id_output = gr.Textbox(label="ä»»åŠ¡ID")
                            task_result = gr.JSON(label="ä»»åŠ¡ç»“æœ")
                    
                    task_list = gr.Dataframe(
                        headers=["ä»»åŠ¡ID", "çŠ¶æ€", "GPU", "å¼€å§‹æ—¶é—´", "æ‰§è¡Œæ—¶é—´"],
                        label="ä»»åŠ¡åˆ—è¡¨"
                    )
                
                # è¿›ç¨‹ç›‘æ§æ ‡ç­¾
                with gr.TabItem("ğŸ” è¿›ç¨‹ç›‘æ§"):
                    process_table = gr.Dataframe(
                        headers=["PID", "è¿›ç¨‹å", "ç”¨æˆ·", "GPU", "æ˜¾å­˜(MB)", "CPU%", "çŠ¶æ€"],
                        label="GPUè¿›ç¨‹åˆ—è¡¨"
                    )
                    
                    with gr.Row():
                        kill_pid = gr.Number(label="PID", precision=0)
                        kill_btn = gr.Button("ç»ˆæ­¢è¿›ç¨‹", variant="stop")
                
                # é…ç½®ç®¡ç†æ ‡ç­¾
                with gr.TabItem("âš™ï¸ é…ç½®ç®¡ç†"):
                    with gr.Row():
                        with gr.Column():
                            min_memory_config = gr.Slider(
                                label="æœ€å°å¯ç”¨æ˜¾å­˜(GB)",
                                minimum=0.5,
                                maximum=8,
                                value=self.gpu_manager.resource_pool.min_free_memory_gb,
                                step=0.5
                            )
                            scheduling_strategy = gr.Dropdown(
                                label="è°ƒåº¦ç­–ç•¥",
                                choices=[s.value for s in SchedulingStrategy],
                                value=self.gpu_manager.scheduler.strategy.value
                            )
                            max_workers_config = gr.Slider(
                                label="æœ€å¤§å·¥ä½œçº¿ç¨‹æ•°",
                                minimum=1,
                                maximum=16,
                                value=self.gpu_manager.max_workers,
                                step=1
                            )
                        
                        with gr.Column():
                            apply_config_btn = gr.Button("åº”ç”¨é…ç½®", variant="primary")
                            config_status = gr.Textbox(label="é…ç½®çŠ¶æ€")
                            
                            export_metrics_btn = gr.Button("å¯¼å‡ºç»Ÿè®¡æ•°æ®")
                            metrics_file = gr.File(label="ç»Ÿè®¡æ•°æ®æ–‡ä»¶")
            
            # å®šä¹‰äº‹ä»¶å¤„ç†å‡½æ•°
            def refresh_status():
                return self.gpu_manager.get_status()
            
            def plot_gpu_usage():
                """ç»˜åˆ¶GPUä½¿ç”¨ç‡å›¾è¡¨"""
                import matplotlib.pyplot as plt
                
                fig, ax = plt.subplots(figsize=(10, 6))
                
                for gpu_idx, history in self.gpu_manager.metrics['gpu_utilization_history'].items():
                    if history:
                        timestamps = [h['timestamp'] for h in history[-100:]]
                        utilizations = [h['utilization'] for h in history[-100:]]
                        ax.plot(timestamps, utilizations, label=f'GPU {gpu_idx}')
                
                ax.set_xlabel('æ—¶é—´')
                ax.set_ylabel('ä½¿ç”¨ç‡ (%)')
                ax.set_title('GPUä½¿ç”¨ç‡è¶‹åŠ¿')
                ax.legend()
                ax.grid(True)
                plt.xticks(rotation=45)
                plt.tight_layout()
                
                return fig
            
            def plot_memory_usage():
                """ç»˜åˆ¶æ˜¾å­˜ä½¿ç”¨å›¾è¡¨"""
                import matplotlib.pyplot as plt
                
                fig, ax = plt.subplots(figsize=(10, 6))
                
                for gpu_idx, history in self.gpu_manager.metrics['gpu_utilization_history'].items():
                    if history:
                        timestamps = [h['timestamp'] for h in history[-100:]]
                        memory_usage = [h['memory_usage'] for h in history[-100:]]
                        ax.plot(timestamps, memory_usage, label=f'GPU {gpu_idx}')
                
                ax.set_xlabel('æ—¶é—´')
                ax.set_ylabel('æ˜¾å­˜ä½¿ç”¨ (%)')
                ax.set_title('æ˜¾å­˜ä½¿ç”¨è¶‹åŠ¿')
                ax.legend()
                ax.grid(True)
                plt.xticks(rotation=45)
                plt.tight_layout()
                
                return fig
            
            def plot_temperature():
                """ç»˜åˆ¶æ¸©åº¦å›¾è¡¨"""
                import matplotlib.pyplot as plt
                
                fig, ax = plt.subplots(figsize=(10, 6))
                
                for gpu_idx, history in self.gpu_manager.metrics['gpu_utilization_history'].items():
                    if history:
                        timestamps = [h['timestamp'] for h in history[-100:]]
                        temperatures = [h['temperature'] for h in history[-100:]]
                        ax.plot(timestamps, temperatures, label=f'GPU {gpu_idx}')
                
                ax.axhline(y=85, color='r', linestyle='--', label='è­¦æˆ’æ¸©åº¦')
                ax.set_xlabel('æ—¶é—´')
                ax.set_ylabel('æ¸©åº¦ (Â°C)')
                ax.set_title('GPUæ¸©åº¦ç›‘æ§')
                ax.legend()
                ax.grid(True)
                plt.xticks(rotation=45)
                plt.tight_layout()
                
                return fig
            
            def plot_power():
                """ç»˜åˆ¶åŠŸè€—å›¾è¡¨"""
                import matplotlib.pyplot as plt
                
                fig, ax = plt.subplots(figsize=(10, 6))
                
                for gpu_idx, history in self.gpu_manager.metrics['gpu_utilization_history'].items():
                    if history:
                        timestamps = [h['timestamp'] for h in history[-100:]]
                        power_usage = [h['power_usage'] for h in history[-100:]]
                        ax.plot(timestamps, power_usage, label=f'GPU {gpu_idx}')
                
                ax.set_xlabel('æ—¶é—´')
                ax.set_ylabel('åŠŸè€— (W)')
                ax.set_title('GPUåŠŸè€—ç›‘æ§')
                ax.legend()
                ax.grid(True)
                plt.xticks(rotation=45)
                plt.tight_layout()
                
                return fig
            
            def submit_task(func_name, args_str, memory, priority_str):
                """æäº¤ä»»åŠ¡"""
                try:
                    # è§£æå‡½æ•°
                    module_name, func_name = func_name.rsplit('.', 1)
                    module = __import__(module_name)
                    func = getattr(module, func_name)
                    
                    # è§£æå‚æ•°
                    args = eval(args_str)
                    if not isinstance(args, tuple):
                        args = (args,)
                    
                    # æäº¤ä»»åŠ¡
                    priority = TaskPriority[priority_str]
                    task_id = self.gpu_manager.submit_task(
                        func=func,
                        args=args,
                        priority=priority,
                        required_memory_gb=memory
                    )
                    
                    return task_id, "ä»»åŠ¡æäº¤æˆåŠŸ"
                    
                except Exception as e:
                    return "", f"é”™è¯¯: {str(e)}"
            
            def get_task_list():
                """è·å–ä»»åŠ¡åˆ—è¡¨"""
                tasks = []
                
                # è·å–å½“å‰æ‰§è¡Œçš„ä»»åŠ¡
                for task in self.gpu_manager.executor.current_tasks.values():
                    tasks.append([
                        task.task_id,
                        task.status.value,
                        task.assigned_gpu or "ç­‰å¾…ä¸­",
                        task.start_time.strftime('%H:%M:%S') if task.start_time else "",
                        str(task.execution_time) if task.execution_time else ""
                    ])
                
                return tasks
            
            def get_process_list():
                """è·å–è¿›ç¨‹åˆ—è¡¨"""
                processes = []
                gpus = self.gpu_manager.resource_pool.get_all_gpu_info()
                
                for gpu in gpus:
                    for proc in gpu.processes:
                        processes.append([
                            proc['pid'],
                            proc['name'],
                            proc['username'],
                            gpu.index,
                            proc['memory_mb'],
                            f"{proc['cpu_percent']:.1f}",
                            proc['status']
                        ])
                
                return processes
            
            def kill_process(pid):
                """ç»ˆæ­¢è¿›ç¨‹"""
                try:
                    if pid:
                        os.kill(int(pid), signal.SIGTERM)
                        return f"å·²å‘é€ç»ˆæ­¢ä¿¡å·åˆ°è¿›ç¨‹ {pid}"
                    return "è¯·è¾“å…¥PID"
                except Exception as e:
                    return f"é”™è¯¯: {str(e)}"
            
            def apply_config(min_mem, strategy, max_workers):
                """åº”ç”¨é…ç½®"""
                try:
                    self.gpu_manager.resource_pool.min_free_memory_gb = min_mem
                    self.gpu_manager.scheduler.strategy = SchedulingStrategy(strategy)
                    # æ³¨æ„ï¼šmax_workerséœ€è¦é‡å¯æ‰èƒ½ç”Ÿæ•ˆ
                    return "é…ç½®å·²æ›´æ–°ï¼ˆéƒ¨åˆ†é…ç½®éœ€è¦é‡å¯ç”Ÿæ•ˆï¼‰"
                except Exception as e:
                    return f"é…ç½®æ›´æ–°å¤±è´¥: {str(e)}"
            
            def export_metrics():
                """å¯¼å‡ºç»Ÿè®¡æ•°æ®"""
                try:
                    import json
                    
                    metrics_data = {
                        'export_time': datetime.now().isoformat(),
                        'metrics': self.gpu_manager.metrics,
                        'status': self.gpu_manager.get_status()
                    }
                    
                    filename = f"gpu_metrics_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
                    with open(filename, 'w') as f:
                        json.dump(metrics_data, f, indent=2, default=str)
                    
                    return filename
                except Exception as e:
                    logger.error(f"Export metrics failed: {e}")
                    return None
            
            # ç»‘å®šäº‹ä»¶
            refresh_btn.click(
                fn=refresh_status,
                outputs=[status_text]
            )
            
            refresh_btn.click(
                fn=plot_gpu_usage,
                outputs=[gpu_usage_plot]
            )
            
            refresh_btn.click(
                fn=plot_memory_usage,
                outputs=[memory_usage_plot]
            )
            
            refresh_btn.click(
                fn=plot_temperature,
                outputs=[temperature_plot]
            )
            
            refresh_btn.click(
                fn=plot_power,
                outputs=[power_plot]
            )
            
            submit_btn.click(
                fn=submit_task,
                inputs=[task_func, task_args, required_memory, priority],
                outputs=[task_id_output, task_result]
            )
            
            refresh_btn.click(
                fn=get_task_list,
                outputs=[task_list]
            )
            
            refresh_btn.click(
                fn=get_process_list,
                outputs=[process_table]
            )
            
            kill_btn.click(
                fn=kill_process,
                inputs=[kill_pid],
                outputs=[config_status]
            )
            
            apply_config_btn.click(
                fn=apply_config,
                inputs=[min_memory_config, scheduling_strategy, max_workers_config],
                outputs=[config_status]
            )
            
            export_metrics_btn.click(
                fn=export_metrics,
                outputs=[metrics_file]
            )
            
            # è‡ªåŠ¨æ›´æ–°
            interface.load(refresh_status, outputs=[status_text])
            
        return interface


# ä¾¿æ·APIå‡½æ•°
_default_manager = None

def init_gpu_manager(**kwargs) -> GPUManager:
    """åˆå§‹åŒ–é»˜è®¤GPUç®¡ç†å™¨"""
    global _default_manager
    _default_manager = GPUManager(**kwargs)
    _default_manager.start()
    return _default_manager

def get_gpu_manager() -> GPUManager:
    """è·å–é»˜è®¤GPUç®¡ç†å™¨"""
    global _default_manager
    if _default_manager is None:
        _default_manager = init_gpu_manager()
    return _default_manager

def gpu_task(required_memory_gb: float = 1.0, 
             priority: TaskPriority = TaskPriority.NORMAL,
             timeout: Optional[float] = None):
    """GPUä»»åŠ¡è£…é¥°å™¨"""
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            manager = get_gpu_manager()
            task_id = manager.submit_task(
                func=func,
                args=args,
                kwargs=kwargs,
                required_memory_gb=required_memory_gb,
                priority=priority,
                timeout=timeout
            )
            return manager.get_task_result(task_id)
        return wrapper
    return decorator


# ç¤ºä¾‹ç”¨æ³•
if __name__ == "__main__":
    # åˆå§‹åŒ–GPUç®¡ç†å™¨
    gpu_manager = GPUManager(
        min_free_memory_gb=2.0,
        scheduling_strategy=SchedulingStrategy.ADAPTIVE,
        max_workers=4
    )
    
    # å¯åŠ¨ç®¡ç†å™¨
    gpu_manager.start()
    
    # æ‰“å°çŠ¶æ€
    gpu_manager.print_status()
    
    # ç¤ºä¾‹ï¼šä½¿ç”¨è£…é¥°å™¨
    @gpu_task(required_memory_gb=1.0, priority=TaskPriority.HIGH)
    def matrix_multiply(size=1000):
        import numpy as np
        a = np.random.randn(size, size)
        b = np.random.randn(size, size)
        return np.dot(a, b)
    
    # æ‰§è¡Œä»»åŠ¡
    # result = matrix_multiply(2000)
    
    # æ‰¹é‡å¤„ç†ç¤ºä¾‹
    # data = list(range(100))
    # results = gpu_manager.batch_process(
    #     data=data,
    #     process_func=lambda x: [i**2 for i in x],
    #     batch_size=10
    # )
    
    # å¯åŠ¨Webç•Œé¢
    if GRADIO_AVAILABLE:
        web_ui = GPUWebUI(gpu_manager)
        interface = web_ui.create_interface()
        if interface:
            interface.launch(server_name="0.0.0.0", server_port=7860, share=True)
    
    # ä¿æŒè¿è¡Œ
    try:
        while True:
            time.sleep(1)
    except KeyboardInterrupt:
        gpu_manager.shutdown()
            